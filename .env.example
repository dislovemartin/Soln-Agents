# SolnAI - Root Environment Configuration Example
# This file serves as a template for environment variables needed across the project.
# Copy this file to .env and customize the values as needed.

# =============================================
# GENERAL CONFIGURATION
# =============================================

# Server Configuration
SERVER_PORT=3001                                   # Port for the backend server
STORAGE_DIR="/app/server/storage"                  # Storage directory for server data
UID='1000'                                         # User ID for Docker containers
GID='1000'                                         # Group ID for Docker containers

# Security
JWT_SECRET="my-random-string-for-seeding"          # Secret for JWT tokens (min 12 chars)
SIG_KEY='passphrase'                               # Signature key (min 32 chars)
SIG_SALT='salt'                                    # Signature salt (min 32 chars)

# Frontend Configuration
VITE_API_BASE='http://localhost:3001/api'          # API base URL for local development
# VITE_API_BASE="https://$CODESPACE_NAME-3001.$GITHUB_CODESPACES_PORT_FORWARDING_DOMAIN/api" # for GitHub Codespaces
# VITE_API_BASE='/api'                             # For non-localhost or Docker deployment

# =============================================
# LLM PROVIDER CONFIGURATION
# =============================================
# Uncomment the section for your preferred LLM provider

# OpenAI
# LLM_PROVIDER='openai'
# OPEN_AI_KEY=
# OPEN_MODEL_PREF='gpt-4o'

# Gemini
# LLM_PROVIDER='gemini'
# GEMINI_API_KEY=
# GEMINI_LLM_MODEL_PREF='gemini-pro'

# Azure OpenAI
# LLM_PROVIDER='azure'
# AZURE_OPENAI_ENDPOINT=
# AZURE_OPENAI_KEY=
# OPEN_MODEL_PREF='my-gpt35-deployment'            # Deployment name on Azure
# EMBEDDING_MODEL_PREF='embedder-model'            # Deployment for embeddings

# Anthropic
# LLM_PROVIDER='anthropic'
# ANTHROPIC_API_KEY=sk-ant-xxxx
# ANTHROPIC_MODEL_PREF='claude-2'

# LM Studio
# LLM_PROVIDER='lmstudio'
# LMSTUDIO_BASE_PATH='http://your-server:1234/v1'
# LMSTUDIO_MODEL_PREF='Loaded from Chat UI'
# LMSTUDIO_MODEL_TOKEN_LIMIT=4096

# Local AI
# LLM_PROVIDER='localai'
# LOCAL_AI_BASE_PATH='http://host.docker.internal:8080/v1'
# LOCAL_AI_MODEL_PREF='luna-ai-llama2'
# LOCAL_AI_MODEL_TOKEN_LIMIT=4096
# LOCAL_AI_API_KEY="sk-123abc"

# Ollama
# LLM_PROVIDER='ollama'
# OLLAMA_BASE_PATH='http://host.docker.internal:11434'
# OLLAMA_MODEL_PREF='llama2'
# OLLAMA_MODEL_TOKEN_LIMIT=4096
# OLLAMA_AUTH_TOKEN='your-ollama-auth-token-here'

# Together AI
# LLM_PROVIDER='togetherai'
# TOGETHER_AI_API_KEY='my-together-ai-key'
# TOGETHER_AI_MODEL_PREF='mistralai/Mixtral-8x7B-Instruct-v0.1'

# Mistral
# LLM_PROVIDER='mistral'
# MISTRAL_API_KEY='example-mistral-ai-api-key'
# MISTRAL_MODEL_PREF='mistral-tiny'

# Perplexity
# LLM_PROVIDER='perplexity'
# PERPLEXITY_API_KEY='my-perplexity-key'
# PERPLEXITY_MODEL_PREF='codellama-34b-instruct'

# OpenRouter
# LLM_PROVIDER='openrouter'
# OPENROUTER_API_KEY='my-openrouter-key'
# OPENROUTER_MODEL_PREF='openrouter/auto'

# HuggingFace
# LLM_PROVIDER='huggingface'
# HUGGING_FACE_LLM_ENDPOINT=https://uuid-here.us-east-1.aws.endpoints.huggingface.cloud
# HUGGING_FACE_LLM_API_KEY=hf_xxxxxx
# HUGGING_FACE_LLM_TOKEN_LIMIT=8000

# Groq
# LLM_PROVIDER='groq'
# GROQ_API_KEY=gsk_abcxyz
# GROQ_MODEL_PREF=llama3-8b-8192

# =============================================
# EMBEDDING CONFIGURATION
# =============================================
# Only needed if your LLM provider doesn't natively support embeddings

# OpenAI Embeddings
# EMBEDDING_ENGINE='openai'
# OPEN_AI_KEY=sk-xxxx
# EMBEDDING_MODEL_PREF='text-embedding-ada-002'

# Azure Embeddings
# EMBEDDING_ENGINE='azure'
# AZURE_OPENAI_ENDPOINT=
# AZURE_OPENAI_KEY=
# EMBEDDING_MODEL_PREF='my-embedder-model'

# Local AI Embeddings
# EMBEDDING_ENGINE='localai'
# EMBEDDING_BASE_PATH='http://localhost:8080/v1'
# EMBEDDING_MODEL_PREF='text-embedding-ada-002'
# EMBEDDING_MODEL_MAX_CHUNK_LENGTH=1000

# Ollama Embeddings
# EMBEDDING_ENGINE='ollama'
# EMBEDDING_BASE_PATH='http://host.docker.internal:11434'
# EMBEDDING_MODEL_PREF='nomic-embed-text:latest'
# EMBEDDING_MODEL_MAX_CHUNK_LENGTH=8192

# =============================================
# VECTOR DATABASE CONFIGURATION
# =============================================
# Uncomment the section for your preferred vector database

# LanceDB (Default)
VECTOR_DB="lancedb"

# Chroma
# VECTOR_DB="chroma"
# CHROMA_ENDPOINT='http://host.docker.internal:8000'
# CHROMA_API_HEADER="X-Api-Key"
# CHROMA_API_KEY="sk-123abc"

# Pinecone
# VECTOR_DB="pinecone"
# PINECONE_API_KEY=
# PINECONE_INDEX=

# Astra DB
# VECTOR_DB="astra"
# ASTRA_DB_APPLICATION_TOKEN=
# ASTRA_DB_ENDPOINT=

# Weaviate
# VECTOR_DB="weaviate"
# WEAVIATE_ENDPOINT="http://localhost:8080"
# WEAVIATE_API_KEY=

# =============================================
# AGENT CONFIGURATION
# =============================================
# Uncomment and configure as needed for specific agent functionality

# Web Search Agents
# AGENT_USE_FUNCTIONS=true

# Tavily
# AGENT_TAVILY_API_KEY=

# Google Search
# AGENT_GSE_KEY=
# AGENT_GSE_CTX=

# SearchApi.io
# AGENT_SEARCHAPI_KEY=

# Serper
# AGENT_SERPER_API_KEY=

# Brave Search
# AGENT_BRAVE_API_KEY=

# Bing Search
# AGENT_BING_API_KEY=

# =============================================
# COLLECTOR CONFIGURATION
# =============================================
# Configuration for the collector component

# Placeholder for collector runtime settings

# =============================================
# ADDITIONAL SERVICES
# =============================================
# Uncomment and configure as needed

# Authentication
# AUTH_TOKEN=                                      # Set to enable authentication
# ALLOW_SELF_SIGNUP=true                           # Allow users to sign up

# Document Processing
# DOCPROCESSING_THREADS=2                          # Number of threads for document processing

# Proxy Configuration
# HTTP_PROXY=                                      # HTTP proxy URL
# HTTPS_PROXY=                                     # HTTPS proxy URL

# =============================================
# NOTES
# =============================================
# - For production deployments, ensure all secrets are properly secured
# - Generate strong random strings for JWT_SECRET, SIG_KEY, and SIG_SALT
# - Refer to component-specific documentation for additional configuration options
